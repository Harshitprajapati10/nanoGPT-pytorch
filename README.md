# NanoGPT-PyTorch  
*A implementation of Bigram LM, Self-Attention, and Transformer models trained on Tiny Shakespeare.*

---

## üìò Description

This repository provides a clean, step-by-step PyTorch implementation of a **nanoGPT-style character-level language model**.  
It begins with the simplest **Bigram Language Model**, then introduces **Self-Attention**, and finally builds a full **Transformer (GPT-style)** architecture.

The project is built for learning the internal working of modern LLMs with a readable, compact codebase.  
Dataset used: **Tiny Shakespeare** (~1MB of text).

---

## üìÇ Features

### ‚úîÔ∏è Bigram Character-Level Model  
A simple model predicting the next character from the previous one.

**Concepts:**
- Character-level vocabulary  
- Token/char embeddings  
- Logits ‚Üí softmax ‚Üí cross-entropy  
- Baseline LM for comparison  

---

### ‚úîÔ∏è Self-Attention Mechanism  
The core idea behind modern Transformers.

**Includes:**
- Scaled dot-product attention  
- Causal masking (autoregressive)  
- Multi-Head Attention  
- Query‚ÄìKey‚ÄìValue projections  
- Attention weights via softmax  

---

### ‚úîÔ∏è Transformer Architecture  
A powerful GPT-style miniature model.

**Components:**
- Token & Positional embeddings  
- Multi-Head Self-Attention  
- Feedforward MLP layers  
- LayerNorm  
- Residual connections  
- Auto-regressive text generation  
- Training with AdamW & LR scheduling  

---


---

## üß† Model Progression

### **1Ô∏è‚É£ Bigram Model**
A minimal LM predicting the next token from the current one.

Learns:
- Basic statistics of character transitions  
- Vocabulary embeddings  
- Cross-entropy loss computation  

---

### **2Ô∏è‚É£ Self-Attention**
Introduces sequence awareness.

Learns:
- Token relationships  
- How to focus on important positions  
- Multiple attention "heads" for patterns  

---

### **3Ô∏è‚É£ Transformer**
A complete GPT-like model.

Features:
- Stacked attention blocks  
- Positional embeddings  
- Feedforward MLP  
- LayerNorm + residual pathways  
- Autoregressive text generation  

---


